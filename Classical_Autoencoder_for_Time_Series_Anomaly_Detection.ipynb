{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline: Classical Autoencoder\n",
        "\n",
        "A classical benchmark for the quantum enhanced autoencoder, following the structure given in \"Applying Quantum Autoencoders for Time Series Anomaly Detection\", published 10/10/2024, by Robin Frehner, Kurt Stockinger.\n",
        "\n",
        "Dataset avaliable here: https://www.cs.ucr.edu/~eamonn/time_series_data_2018/ upload it on your Google Drive to make the code work."
      ],
      "metadata": {
        "id": "maoJa9i5nwkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "VY7_h12yatgD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A83vVFf1nMjO",
        "outputId": "30dcba30-5b81-44ad-8475-ed409ebd1168"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to your dataset\n",
        "train_file_path = '/content/drive/My Drive/UCRArchive_2018/Fish/Fish_TRAIN.tsv'\n",
        "test_file_path = '/content/drive/My Drive/UCRArchive_2018/Fish/Fish_TEST.tsv'"
      ],
      "metadata": {
        "id": "xOOjS0-7axWs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "def load_data(file_path):\n",
        "    data = pd.read_csv(file_path, sep='\\t', header=None)\n",
        "    data = data.iloc[:, 1:]  # Remove the first column (it's extra info we don't use)\n",
        "    return data.values\n",
        "\n",
        "train_data = load_data(train_file_path)\n",
        "test_data = load_data(test_file_path)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "train_data = scaler.fit_transform(train_data)\n",
        "test_data = scaler.transform(test_data)\n"
      ],
      "metadata": {
        "id": "9ryGwQ7jkRpJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data into sliding windows\n",
        "window_size = 128\n",
        "stride = 1\n",
        "\n",
        "def create_sliding_windows(data, window_size, stride):\n",
        "    windows = []\n",
        "    for i in range(0, data.shape[0] - window_size + 1, stride):\n",
        "        window = data[i:i + window_size]\n",
        "        windows.append(window)\n",
        "    return np.array(windows)\n",
        "\n",
        "train_windows = create_sliding_windows(train_data, window_size, stride)\n",
        "test_windows = create_sliding_windows(test_data, window_size, stride)\n",
        "\n",
        "# Verify dimensions\n",
        "print(\"Train windows shape:\", train_windows.shape)\n",
        "print(\"Test windows shape:\", test_windows.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxwA7XSbkU5w",
        "outputId": "8e993024-6235-45a0-99fe-a6ce8de67cea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train windows shape: (48, 128, 463)\n",
            "Test windows shape: (48, 128, 463)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch Dataset and DataLoader\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "train_dataset = TimeSeriesDataset(train_windows)\n",
        "test_dataset = TimeSeriesDataset(test_windows)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=150, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=150, shuffle=False)\n"
      ],
      "metadata": {
        "id": "eo5V9yGWkaPc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, window_size, input_size):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder_lstm = nn.LSTM(input_size=input_size, hidden_size=64, batch_first=True)\n",
        "        self.encoder_fc = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(10, 4)\n",
        "        )\n",
        "        self.decoder_fc = nn.Sequential(\n",
        "            nn.Linear(4, 10),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(10, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder_lstm = nn.LSTM(input_size=64, hidden_size=input_size, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, sequence_length, feature_size = x.size()\n",
        "\n",
        "        # Encoding\n",
        "        x, _ = self.encoder_lstm(x)  # (batch_size, sequence_length, hidden_size)\n",
        "        x = self.encoder_fc(x[:, -1, :])  # (batch_size, latent_dim)\n",
        "\n",
        "        # Decoding\n",
        "        x = self.decoder_fc(x)  # (batch_size, hidden_size)\n",
        "        x = x.unsqueeze(1).repeat(1, sequence_length, 1)  # Repeat for sequence length\n",
        "        x, _ = self.decoder_lstm(x)  # (batch_size, sequence_length, input_size)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "1jcM1qojkdb2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss, and optimizer\n",
        "input_size = train_windows.shape[2]  # 80 features per time step\n",
        "model = Autoencoder(window_size=window_size, input_size=input_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=250):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch)\n",
        "            loss = criterion(outputs, batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Train the autoencoder\n",
        "train_model(model, train_loader, criterion, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sXV2j2akgl3",
        "outputId": "c18549e5-3bba-47ea-b51e-44c749e2b76e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250, Loss: 0.2786\n",
            "Epoch 2/250, Loss: 0.2693\n",
            "Epoch 3/250, Loss: 0.2585\n",
            "Epoch 4/250, Loss: 0.2434\n",
            "Epoch 5/250, Loss: 0.2157\n",
            "Epoch 6/250, Loss: 0.1288\n",
            "Epoch 7/250, Loss: 0.1298\n",
            "Epoch 8/250, Loss: 0.1228\n",
            "Epoch 9/250, Loss: 0.1061\n",
            "Epoch 10/250, Loss: 0.0863\n",
            "Epoch 11/250, Loss: 0.0697\n",
            "Epoch 12/250, Loss: 0.0603\n",
            "Epoch 13/250, Loss: 0.0577\n",
            "Epoch 14/250, Loss: 0.0566\n",
            "Epoch 15/250, Loss: 0.0543\n",
            "Epoch 16/250, Loss: 0.0513\n",
            "Epoch 17/250, Loss: 0.0487\n",
            "Epoch 18/250, Loss: 0.0466\n",
            "Epoch 19/250, Loss: 0.0448\n",
            "Epoch 20/250, Loss: 0.0433\n",
            "Epoch 21/250, Loss: 0.0419\n",
            "Epoch 22/250, Loss: 0.0406\n",
            "Epoch 23/250, Loss: 0.0395\n",
            "Epoch 24/250, Loss: 0.0386\n",
            "Epoch 25/250, Loss: 0.0380\n",
            "Epoch 26/250, Loss: 0.0377\n",
            "Epoch 27/250, Loss: 0.0376\n",
            "Epoch 28/250, Loss: 0.0374\n",
            "Epoch 29/250, Loss: 0.0372\n",
            "Epoch 30/250, Loss: 0.0368\n",
            "Epoch 31/250, Loss: 0.0365\n",
            "Epoch 32/250, Loss: 0.0361\n",
            "Epoch 33/250, Loss: 0.0357\n",
            "Epoch 34/250, Loss: 0.0354\n",
            "Epoch 35/250, Loss: 0.0351\n",
            "Epoch 36/250, Loss: 0.0350\n",
            "Epoch 37/250, Loss: 0.0348\n",
            "Epoch 38/250, Loss: 0.0347\n",
            "Epoch 39/250, Loss: 0.0346\n",
            "Epoch 40/250, Loss: 0.0345\n",
            "Epoch 41/250, Loss: 0.0344\n",
            "Epoch 42/250, Loss: 0.0343\n",
            "Epoch 43/250, Loss: 0.0342\n",
            "Epoch 44/250, Loss: 0.0341\n",
            "Epoch 45/250, Loss: 0.0340\n",
            "Epoch 46/250, Loss: 0.0339\n",
            "Epoch 47/250, Loss: 0.0338\n",
            "Epoch 48/250, Loss: 0.0337\n",
            "Epoch 49/250, Loss: 0.0336\n",
            "Epoch 50/250, Loss: 0.0336\n",
            "Epoch 51/250, Loss: 0.0335\n",
            "Epoch 52/250, Loss: 0.0334\n",
            "Epoch 53/250, Loss: 0.0334\n",
            "Epoch 54/250, Loss: 0.0333\n",
            "Epoch 55/250, Loss: 0.0332\n",
            "Epoch 56/250, Loss: 0.0331\n",
            "Epoch 57/250, Loss: 0.0331\n",
            "Epoch 58/250, Loss: 0.0330\n",
            "Epoch 59/250, Loss: 0.0330\n",
            "Epoch 60/250, Loss: 0.0329\n",
            "Epoch 61/250, Loss: 0.0329\n",
            "Epoch 62/250, Loss: 0.0329\n",
            "Epoch 63/250, Loss: 0.0328\n",
            "Epoch 64/250, Loss: 0.0328\n",
            "Epoch 65/250, Loss: 0.0327\n",
            "Epoch 66/250, Loss: 0.0327\n",
            "Epoch 67/250, Loss: 0.0326\n",
            "Epoch 68/250, Loss: 0.0326\n",
            "Epoch 69/250, Loss: 0.0326\n",
            "Epoch 70/250, Loss: 0.0325\n",
            "Epoch 71/250, Loss: 0.0325\n",
            "Epoch 72/250, Loss: 0.0325\n",
            "Epoch 73/250, Loss: 0.0324\n",
            "Epoch 74/250, Loss: 0.0324\n",
            "Epoch 75/250, Loss: 0.0324\n",
            "Epoch 76/250, Loss: 0.0324\n",
            "Epoch 77/250, Loss: 0.0323\n",
            "Epoch 78/250, Loss: 0.0323\n",
            "Epoch 79/250, Loss: 0.0323\n",
            "Epoch 80/250, Loss: 0.0323\n",
            "Epoch 81/250, Loss: 0.0323\n",
            "Epoch 82/250, Loss: 0.0322\n",
            "Epoch 83/250, Loss: 0.0322\n",
            "Epoch 84/250, Loss: 0.0322\n",
            "Epoch 85/250, Loss: 0.0322\n",
            "Epoch 86/250, Loss: 0.0322\n",
            "Epoch 87/250, Loss: 0.0322\n",
            "Epoch 88/250, Loss: 0.0322\n",
            "Epoch 89/250, Loss: 0.0321\n",
            "Epoch 90/250, Loss: 0.0321\n",
            "Epoch 91/250, Loss: 0.0321\n",
            "Epoch 92/250, Loss: 0.0321\n",
            "Epoch 93/250, Loss: 0.0321\n",
            "Epoch 94/250, Loss: 0.0321\n",
            "Epoch 95/250, Loss: 0.0321\n",
            "Epoch 96/250, Loss: 0.0321\n",
            "Epoch 97/250, Loss: 0.0321\n",
            "Epoch 98/250, Loss: 0.0321\n",
            "Epoch 99/250, Loss: 0.0321\n",
            "Epoch 100/250, Loss: 0.0321\n",
            "Epoch 101/250, Loss: 0.0320\n",
            "Epoch 102/250, Loss: 0.0320\n",
            "Epoch 103/250, Loss: 0.0320\n",
            "Epoch 104/250, Loss: 0.0320\n",
            "Epoch 105/250, Loss: 0.0320\n",
            "Epoch 106/250, Loss: 0.0320\n",
            "Epoch 107/250, Loss: 0.0320\n",
            "Epoch 108/250, Loss: 0.0320\n",
            "Epoch 109/250, Loss: 0.0320\n",
            "Epoch 110/250, Loss: 0.0320\n",
            "Epoch 111/250, Loss: 0.0320\n",
            "Epoch 112/250, Loss: 0.0320\n",
            "Epoch 113/250, Loss: 0.0320\n",
            "Epoch 114/250, Loss: 0.0320\n",
            "Epoch 115/250, Loss: 0.0320\n",
            "Epoch 116/250, Loss: 0.0320\n",
            "Epoch 117/250, Loss: 0.0320\n",
            "Epoch 118/250, Loss: 0.0320\n",
            "Epoch 119/250, Loss: 0.0320\n",
            "Epoch 120/250, Loss: 0.0320\n",
            "Epoch 121/250, Loss: 0.0320\n",
            "Epoch 122/250, Loss: 0.0320\n",
            "Epoch 123/250, Loss: 0.0320\n",
            "Epoch 124/250, Loss: 0.0320\n",
            "Epoch 125/250, Loss: 0.0319\n",
            "Epoch 126/250, Loss: 0.0319\n",
            "Epoch 127/250, Loss: 0.0319\n",
            "Epoch 128/250, Loss: 0.0319\n",
            "Epoch 129/250, Loss: 0.0319\n",
            "Epoch 130/250, Loss: 0.0319\n",
            "Epoch 131/250, Loss: 0.0319\n",
            "Epoch 132/250, Loss: 0.0319\n",
            "Epoch 133/250, Loss: 0.0318\n",
            "Epoch 134/250, Loss: 0.0318\n",
            "Epoch 135/250, Loss: 0.0318\n",
            "Epoch 136/250, Loss: 0.0317\n",
            "Epoch 137/250, Loss: 0.0317\n",
            "Epoch 138/250, Loss: 0.0317\n",
            "Epoch 139/250, Loss: 0.0317\n",
            "Epoch 140/250, Loss: 0.0317\n",
            "Epoch 141/250, Loss: 0.0317\n",
            "Epoch 142/250, Loss: 0.0317\n",
            "Epoch 143/250, Loss: 0.0317\n",
            "Epoch 144/250, Loss: 0.0317\n",
            "Epoch 145/250, Loss: 0.0317\n",
            "Epoch 146/250, Loss: 0.0317\n",
            "Epoch 147/250, Loss: 0.0317\n",
            "Epoch 148/250, Loss: 0.0317\n",
            "Epoch 149/250, Loss: 0.0317\n",
            "Epoch 150/250, Loss: 0.0317\n",
            "Epoch 151/250, Loss: 0.0317\n",
            "Epoch 152/250, Loss: 0.0317\n",
            "Epoch 153/250, Loss: 0.0317\n",
            "Epoch 154/250, Loss: 0.0317\n",
            "Epoch 155/250, Loss: 0.0317\n",
            "Epoch 156/250, Loss: 0.0317\n",
            "Epoch 157/250, Loss: 0.0317\n",
            "Epoch 158/250, Loss: 0.0317\n",
            "Epoch 159/250, Loss: 0.0317\n",
            "Epoch 160/250, Loss: 0.0317\n",
            "Epoch 161/250, Loss: 0.0317\n",
            "Epoch 162/250, Loss: 0.0317\n",
            "Epoch 163/250, Loss: 0.0317\n",
            "Epoch 164/250, Loss: 0.0317\n",
            "Epoch 165/250, Loss: 0.0317\n",
            "Epoch 166/250, Loss: 0.0317\n",
            "Epoch 167/250, Loss: 0.0317\n",
            "Epoch 168/250, Loss: 0.0317\n",
            "Epoch 169/250, Loss: 0.0317\n",
            "Epoch 170/250, Loss: 0.0317\n",
            "Epoch 171/250, Loss: 0.0317\n",
            "Epoch 172/250, Loss: 0.0317\n",
            "Epoch 173/250, Loss: 0.0317\n",
            "Epoch 174/250, Loss: 0.0317\n",
            "Epoch 175/250, Loss: 0.0317\n",
            "Epoch 176/250, Loss: 0.0317\n",
            "Epoch 177/250, Loss: 0.0317\n",
            "Epoch 178/250, Loss: 0.0317\n",
            "Epoch 179/250, Loss: 0.0317\n",
            "Epoch 180/250, Loss: 0.0317\n",
            "Epoch 181/250, Loss: 0.0317\n",
            "Epoch 182/250, Loss: 0.0317\n",
            "Epoch 183/250, Loss: 0.0317\n",
            "Epoch 184/250, Loss: 0.0317\n",
            "Epoch 185/250, Loss: 0.0317\n",
            "Epoch 186/250, Loss: 0.0317\n",
            "Epoch 187/250, Loss: 0.0317\n",
            "Epoch 188/250, Loss: 0.0317\n",
            "Epoch 189/250, Loss: 0.0317\n",
            "Epoch 190/250, Loss: 0.0317\n",
            "Epoch 191/250, Loss: 0.0317\n",
            "Epoch 192/250, Loss: 0.0317\n",
            "Epoch 193/250, Loss: 0.0317\n",
            "Epoch 194/250, Loss: 0.0317\n",
            "Epoch 195/250, Loss: 0.0317\n",
            "Epoch 196/250, Loss: 0.0317\n",
            "Epoch 197/250, Loss: 0.0317\n",
            "Epoch 198/250, Loss: 0.0317\n",
            "Epoch 199/250, Loss: 0.0317\n",
            "Epoch 200/250, Loss: 0.0317\n",
            "Epoch 201/250, Loss: 0.0317\n",
            "Epoch 202/250, Loss: 0.0317\n",
            "Epoch 203/250, Loss: 0.0317\n",
            "Epoch 204/250, Loss: 0.0317\n",
            "Epoch 205/250, Loss: 0.0317\n",
            "Epoch 206/250, Loss: 0.0317\n",
            "Epoch 207/250, Loss: 0.0317\n",
            "Epoch 208/250, Loss: 0.0317\n",
            "Epoch 209/250, Loss: 0.0317\n",
            "Epoch 210/250, Loss: 0.0317\n",
            "Epoch 211/250, Loss: 0.0317\n",
            "Epoch 212/250, Loss: 0.0317\n",
            "Epoch 213/250, Loss: 0.0317\n",
            "Epoch 214/250, Loss: 0.0317\n",
            "Epoch 215/250, Loss: 0.0317\n",
            "Epoch 216/250, Loss: 0.0317\n",
            "Epoch 217/250, Loss: 0.0317\n",
            "Epoch 218/250, Loss: 0.0317\n",
            "Epoch 219/250, Loss: 0.0317\n",
            "Epoch 220/250, Loss: 0.0317\n",
            "Epoch 221/250, Loss: 0.0317\n",
            "Epoch 222/250, Loss: 0.0317\n",
            "Epoch 223/250, Loss: 0.0317\n",
            "Epoch 224/250, Loss: 0.0317\n",
            "Epoch 225/250, Loss: 0.0316\n",
            "Epoch 226/250, Loss: 0.0316\n",
            "Epoch 227/250, Loss: 0.0316\n",
            "Epoch 228/250, Loss: 0.0316\n",
            "Epoch 229/250, Loss: 0.0316\n",
            "Epoch 230/250, Loss: 0.0316\n",
            "Epoch 231/250, Loss: 0.0316\n",
            "Epoch 232/250, Loss: 0.0316\n",
            "Epoch 233/250, Loss: 0.0316\n",
            "Epoch 234/250, Loss: 0.0316\n",
            "Epoch 235/250, Loss: 0.0316\n",
            "Epoch 236/250, Loss: 0.0316\n",
            "Epoch 237/250, Loss: 0.0316\n",
            "Epoch 238/250, Loss: 0.0316\n",
            "Epoch 239/250, Loss: 0.0316\n",
            "Epoch 240/250, Loss: 0.0316\n",
            "Epoch 241/250, Loss: 0.0316\n",
            "Epoch 242/250, Loss: 0.0316\n",
            "Epoch 243/250, Loss: 0.0316\n",
            "Epoch 244/250, Loss: 0.0316\n",
            "Epoch 245/250, Loss: 0.0316\n",
            "Epoch 246/250, Loss: 0.0316\n",
            "Epoch 247/250, Loss: 0.0316\n",
            "Epoch 248/250, Loss: 0.0316\n",
            "Epoch 249/250, Loss: 0.0316\n",
            "Epoch 250/250, Loss: 0.0316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on training data to find a threshold\n",
        "def evaluate_model_train(model, train_loader, criterion):\n",
        "    model.eval()\n",
        "    reconstruction_losses = []\n",
        "    with torch.no_grad():\n",
        "        for batch in train_loader:\n",
        "            outputs = model(batch)\n",
        "            loss = criterion(outputs, batch)\n",
        "            reconstruction_losses.append(loss.item())\n",
        "    return np.array(reconstruction_losses)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "def evaluate_model_test(model, test_loader):\n",
        "    model.eval()\n",
        "    reconstruction_losses = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            outputs = model(batch)\n",
        "            loss = criterion(outputs, batch)\n",
        "            reconstruction_losses.append(loss.item())\n",
        "    return np.array(reconstruction_losses)\n",
        "\n",
        "\n",
        "train_losses = evaluate_model_train(model, train_loader, criterion)\n",
        "threshold = np.percentile(train_losses, 95) # Find the threshold on the train data\n",
        "\n",
        "test_losses = evaluate_model_test(model, test_loader)\n",
        "\n",
        "# Identify the single anomaly as the test instance with the highest loss\n",
        "anomaly_index = np.argmax(test_losses)\n",
        "anomalies = np.zeros_like(test_losses, dtype=bool)\n",
        "anomalies[anomaly_index] = True\n",
        "\n",
        "# Save results\n",
        "results = pd.DataFrame({\n",
        "    'Reconstruction Loss': test_losses,\n",
        "    'Anomaly': anomalies\n",
        "})\n",
        "results.to_csv('/content/drive/My Drive/anomaly_results.csv', index=False)\n",
        "\n",
        "print(\"Threshold for anomalies:\", threshold)\n",
        "print(\"Anomalies detected:\", np.sum(anomalies))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P9jZVsHtP3T",
        "outputId": "bd584adc-15e8-4c55-d636-065c09cc6178"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold for anomalies: 0.03479915112257004\n",
            "Anomalies detected: 1\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Classical Autoencoder for Time Series Anomaly Detection.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}