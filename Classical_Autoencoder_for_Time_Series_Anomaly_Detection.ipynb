{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline: Classical Autoencoder\n",
        "A classical benchmark for the quantum enhanced autoencoder, following the structure given in \"Applying Quantum Autoencoders for Time Series Anomaly Detection\", published 10/10/2024, by Robin Frehner, Kurt Stockinger."
      ],
      "metadata": {
        "id": "maoJa9i5nwkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "VY7_h12yatgD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A83vVFf1nMjO",
        "outputId": "67f8eb9a-a8be-4201-b883-2fa6d321b23b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to your dataset\n",
        "train_file_path = '/content/drive/My Drive/UCRArchive_2018/Fish/Fish_TRAIN.tsv'\n",
        "test_file_path = '/content/drive/My Drive/UCRArchive_2018/Fish/Fish_TEST.tsv'"
      ],
      "metadata": {
        "id": "xOOjS0-7axWs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "def load_data(file_path):\n",
        "    data = pd.read_csv(file_path, sep='\\t', header=None)\n",
        "    data = data.iloc[:, 1:]  # Remove the first useless column\n",
        "    return data.values\n",
        "\n",
        "train_data = load_data(train_file_path)\n",
        "test_data = load_data(test_file_path)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "train_data = scaler.fit_transform(train_data)\n",
        "test_data = scaler.transform(test_data)\n"
      ],
      "metadata": {
        "id": "9ryGwQ7jkRpJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data into sliding windows\n",
        "window_size = 128\n",
        "stride = 1\n",
        "\n",
        "def create_sliding_windows(data, window_size, stride):\n",
        "    windows = []\n",
        "    for i in range(0, data.shape[0] - window_size + 1, stride):\n",
        "        window = data[i:i + window_size]\n",
        "        windows.append(window)\n",
        "    return np.array(windows)\n",
        "\n",
        "train_windows = create_sliding_windows(train_data, window_size, stride)\n",
        "test_windows = create_sliding_windows(test_data, window_size, stride)\n",
        "\n",
        "# Verify dimensions\n",
        "print(\"Train windows shape:\", train_windows.shape)\n",
        "print(\"Test windows shape:\", test_windows.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxwA7XSbkU5w",
        "outputId": "2027d4cb-f5e9-4f89-c9a6-4ced40054e74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train windows shape: (48, 128, 463)\n",
            "Test windows shape: (48, 128, 463)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch Dataset and DataLoader\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "train_dataset = TimeSeriesDataset(train_windows)\n",
        "test_dataset = TimeSeriesDataset(test_windows)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "eo5V9yGWkaPc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, window_size, input_size):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder_lstm = nn.LSTM(input_size=input_size, hidden_size=128, batch_first=True)\n",
        "        self.encoder_fc = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(10, 4)\n",
        "        )\n",
        "        self.decoder_fc = nn.Sequential(\n",
        "            nn.Linear(4, 10),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(10, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder_lstm = nn.LSTM(input_size=128, hidden_size=input_size, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, sequence_length, feature_size = x.size()\n",
        "\n",
        "        # Encoding\n",
        "        x, _ = self.encoder_lstm(x)  # (batch_size, sequence_length, hidden_size)\n",
        "        x = self.encoder_fc(x[:, -1, :])  # (batch_size, latent_dim)\n",
        "\n",
        "        # Decoding\n",
        "        x = self.decoder_fc(x)  # (batch_size, hidden_size)\n",
        "        x = x.unsqueeze(1).repeat(1, sequence_length, 1)  # Repeat for sequence length\n",
        "        x, _ = self.decoder_lstm(x)  # (batch_size, sequence_length, input_size)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "1jcM1qojkdb2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss, and optimizer\n",
        "input_size = train_windows.shape[2]  # 80 features per time step\n",
        "model = Autoencoder(window_size=window_size, input_size=input_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=50):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch)\n",
        "            loss = criterion(outputs, batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Train the autoencoder\n",
        "train_model(model, train_loader, criterion, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sXV2j2akgl3",
        "outputId": "082868a1-bf80-4169-d79c-32952184d079"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 0.2732\n",
            "Epoch 2/50, Loss: 0.2434\n",
            "Epoch 3/50, Loss: 0.1621\n",
            "Epoch 4/50, Loss: 0.1175\n",
            "Epoch 5/50, Loss: 0.0864\n",
            "Epoch 6/50, Loss: 0.0579\n",
            "Epoch 7/50, Loss: 0.0530\n",
            "Epoch 8/50, Loss: 0.0494\n",
            "Epoch 9/50, Loss: 0.0450\n",
            "Epoch 10/50, Loss: 0.0425\n",
            "Epoch 11/50, Loss: 0.0406\n",
            "Epoch 12/50, Loss: 0.0391\n",
            "Epoch 13/50, Loss: 0.0381\n",
            "Epoch 14/50, Loss: 0.0374\n",
            "Epoch 15/50, Loss: 0.0366\n",
            "Epoch 16/50, Loss: 0.0361\n",
            "Epoch 17/50, Loss: 0.0358\n",
            "Epoch 18/50, Loss: 0.0355\n",
            "Epoch 19/50, Loss: 0.0352\n",
            "Epoch 20/50, Loss: 0.0350\n",
            "Epoch 21/50, Loss: 0.0348\n",
            "Epoch 22/50, Loss: 0.0347\n",
            "Epoch 23/50, Loss: 0.0345\n",
            "Epoch 24/50, Loss: 0.0343\n",
            "Epoch 25/50, Loss: 0.0341\n",
            "Epoch 26/50, Loss: 0.0340\n",
            "Epoch 27/50, Loss: 0.0339\n",
            "Epoch 28/50, Loss: 0.0338\n",
            "Epoch 29/50, Loss: 0.0337\n",
            "Epoch 30/50, Loss: 0.0336\n",
            "Epoch 31/50, Loss: 0.0335\n",
            "Epoch 32/50, Loss: 0.0334\n",
            "Epoch 33/50, Loss: 0.0332\n",
            "Epoch 34/50, Loss: 0.0332\n",
            "Epoch 35/50, Loss: 0.0330\n",
            "Epoch 36/50, Loss: 0.0330\n",
            "Epoch 37/50, Loss: 0.0328\n",
            "Epoch 38/50, Loss: 0.0327\n",
            "Epoch 39/50, Loss: 0.0326\n",
            "Epoch 40/50, Loss: 0.0324\n",
            "Epoch 41/50, Loss: 0.0323\n",
            "Epoch 42/50, Loss: 0.0323\n",
            "Epoch 43/50, Loss: 0.0322\n",
            "Epoch 44/50, Loss: 0.0321\n",
            "Epoch 45/50, Loss: 0.0320\n",
            "Epoch 46/50, Loss: 0.0320\n",
            "Epoch 47/50, Loss: 0.0320\n",
            "Epoch 48/50, Loss: 0.0319\n",
            "Epoch 49/50, Loss: 0.0319\n",
            "Epoch 50/50, Loss: 0.0318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on test data\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    reconstruction_losses = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            outputs = model(batch)\n",
        "            loss = criterion(outputs, batch)\n",
        "            reconstruction_losses.append(loss.item())\n",
        "    return np.array(reconstruction_losses)\n",
        "\n",
        "reconstruction_losses = evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "id": "Gg3W0LN1lCQY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a threshold for anomalies\n",
        "threshold = np.percentile(reconstruction_losses, 90)\n",
        "\n",
        "# Identify anomalies\n",
        "anomalies = reconstruction_losses > threshold\n",
        "\n",
        "# Save results\n",
        "results = pd.DataFrame({\n",
        "    'Reconstruction Loss': reconstruction_losses,\n",
        "    'Anomaly': anomalies\n",
        "})\n",
        "results.to_csv('/content/drive/My Drive/anomaly_results.csv', index=False)\n",
        "\n",
        "print(\"Threshold for anomalies:\", threshold)\n",
        "print(\"Anomalies detected:\", np.sum(anomalies))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6i2u5jpkMEG",
        "outputId": "61b75382-6180-4f08-af88-ce7956a25705"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold for anomalies: 0.035559603944420815\n",
            "Anomalies detected: 1\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}